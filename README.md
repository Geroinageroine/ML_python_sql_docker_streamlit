# Микросервис парсинга данных на *SQL + Docker + Python + Streamlit*

# Описание

 Проект разбит на несколько частей:
  1. Парсинг сайта hh.ru с вакансиями по DA/DS/DE и сохранение данных (вилка, скилы, город и т.п.) в базу данных MySQL
  2. Обучение моделей (**Линейная регрессия** и **Случайный лес с сеткой лучших параметров**) на определение твоей ИТ-роли и будет ли зарплата выше средней по Москве.
  3. Обернуто это во **Streamlit** + c простым UI
  4. Все собрано через **Docker**. 

  Практический опыт по сбору данных + SQL командам + Python + Streamlit + Docker, тренировка ML на сырых данных + В итоге будешь знать, сколько мне платить при собесе и какой у меня уровень

# Этапы
1. Парсинг данных с hh.ru
2. Добавляем загруженные данные в БД MySQL
3. Загрузка и предподготовка данных
4. Обучение ML модели
5. UI интерфейс
    1. Построим сколько получают чуваки от джуна до сеньёра
    2. Построим Облако слов с важными скиллами
    3. Предсказания ИТ роли (Стажер, Джун и т.д.) и Зарплаты
6. Все обернуто в Streamlit: закэшированны ML модели и данные (В облако загружен файл streamlit_app.py без выгрузки данных SQL)

Микросервис парсинга данных на SQL + Docker + Python + Streamlit

# **Формат входных данных**

Мы обучили модель, оставив данные из hh.ru:

Итоговые данные:
- 'id' - id
- 'name' - краткое название работы
- 'month', 'year' - публикация   объявления
- 'city' - город
- 'experience_year' - минимальный  количество опыта работы
- 'salary_from' - минимальная  зарплата
- 'salary_to' - максимальная   зарплата
- 'salary_avg' - средняя зарплата
- 'role' - полное название работы
- 'responsibility' - скиллы для  работы типа Питона и Экселя

Записываем данные в БД MySQL и через год модель будет обучаться на большем количестве данных

# Начальные задачи, которые появлялись в процессе написания кода

Python:
1. Определение Y 
    1. Зарплата больше средней по Москве (161000 рублей)
    2. Роль iT (стажер, джун, мидл, сеньёр)
2. Выделяем важные признаки и предобрабатываем данные
3. Как сохранить из csv в БД MySQL
4. Как считывать из БД - (# TODO: Это не было сделано в текущем проекте)
5. Модель Линейной регрессии для предсказания iT роли и Случайный лес с сеткой параметров для предсказания зарплаты
6. Все обернуть в Streamlit
    1. Кэши для загрузки файлов и моделей ML
    2. Выгрузить в облако Streamlit приложение (для этого создана папка streamlit_cloud). Лучше загружать в облако файл без доп папок (ТОЛЬКО с рабочей папкой, в которой находится streamlit_app.py). В облаке создается рабочая папка /streamlit_cloud. Поэтому все папки в программе нужно будет переопределять

MySQL:
1. Как импортировать из csv в БД в DBeaver 
2. DBeaver при импорте не понимает Bool значения, поэтому пункт 3.
3. Мы записываем в БД с помощью библиотеки SQLAlchemy + pymysql, которые связывают [python, pandas, MySQL]
4. Куда Docker SQL сохраняет данные? -> определим папку сохранения

Docker:
1. Загрузка на ОС 
2. Docker образ MySQL - он нужен чтобы предоставить доступ пользователью dba с root возможностями
3. Docker python - подключается к внутренней сети docker dns
4. Загрузка всего приложения через Docker-compose

Git
1. Загрузка на гитхаб

# Файлы и папки
1. В папке **ML_python_streamlit_sqlalchemy** 
    1. Содержатся спарсенные данные с hh.ru (Data Analyst2024-12-01.csv) и данные последнего запуска программы (date.txt)
    2. db_connect_append.py - подключается к БД и загружает данные в нее
    3. hh_parser.py - парсит данные с hh.ru и сохраняет в файл *.csv
    4. main.py - основная программа Streamlit **frontend** + Python **backend**
2. В папке docker-steps - оставил папку для просмотра (в ней нет функционала для приложения)
    1. docker-hist-command.txt - команды которые использовались (мусор)
    2. docker-sql-init.txt - шпаргалка: docker загрузка на ОС, docker образ SQL, docker Python + Streamlit
3. streamlit_cloud - Папка для загрузки в облако без БД SQL


# Что нужно доделать?
1. Лучше обучить модели на большем количестве данных
    1. При обучении использовать Пайплайны
2. Сразу записывать в SQL все данные (спарсенные и обработанные) и проверять дубликаты по id
3. Задача которая не выполнена: Как считывать из БД - (# TODO: Это не было сделано в текущем проекте)
4. Записывать дату парсинга в MySQL
5. Может быть приделать языковые модели для предобработки данных
6. Парсить среднюю зарплату по Москве (сейчас используется константа)